1) 기술적 도전 경험

어려운 기술적 문제를 해결했던 하나의 경험을 자세히 알려주세요. 대용량 트래픽 처리, 또는 결제 시스템 관련 경험이라면 더욱 좋습니다.

- 문제 상황, 접근 방법, 그리고 결과를 코드 예시와 함께 구체적으로 설명해 주세요.

- 완벽하게 해결하지 못한 문제라도 당신의 분석과 접근 방식을 보여주세요.

=> 

# 배달 시간 예측 모델 실시간 서빙 서비스 구축 프로젝트

## 활용 기술

- 인프라/클라우드: GCP, k8s(GKE), Container, Helm, skaffold, Terraform, Lens
- 백엔드: FastAPI(Python), poetry, pandas
- 프론트엔드: React, TypeScript, CSS(LESS), NextJS, EChart, Material UI
- 데이터베이스: BigQuery, PostgreSQL, Kafka(Confluent Cloud), Redis
- CI/CD: Github Action, ArgoCD, Google Cloud Container/Artifact Registry
- 모니터링/기타 도구: Google Cloud Logging, Datadog, Airflow, Jupyter Notebook, Github, Locust

## 본인의 역할

- Team Lead, Project Tech Lead
- 기술적 의사 결정(오픈소스 검토, 기술 스택 결정, 코딩 표준 및 스타일 가이드 설정, 코드베이스 구조 설계 등)
- 아키텍처 설계, 데이터베이스 설계, 백엔드 개발
- PO, ML 엔지니어와 개발자들 간의 기술적 가교 역할 수행
- ML 모델을 실시간으로 서빙하기 위한 API 설계 및 개발
- 가게 설정 화면 기획 및 프론트엔드 개발 가이드
- 대용량 트래픽 처리를 위한 시스템 아키텍처 설계 및 구현
- 모니터링을 통한 지속적인 성능 개선 및 최적화
- 서비스 안정성 확보를 위한 장애 대응 및 복구 전략 수립

## 진행 방식

- 2주 단위의 스프린트로 애자일/스크럼 방식 채택
- 백엔드 개발자, 프론트엔드 개발자, Data/MLOps 엔지니어로 구성된 팀 운영
- 주 2회 ML 팀과 서비스 개발 팀 간의 기술 공유 세션 진행
- 지속적 통합/배포/학습(CI/CD/CT) 파이프라인을 구축하여 모델 및 서비스 업데이트 자동화

## 문제 상황

실시간 처리 지연 문제: 피크 시간대에 초당 수백 건의 예측 요청이 들어오면서 응답 시간이 크게 증가하는 문제가 발생했습니다.

## 접근 방법

파이썬으로 개발된 마이크로 서비스 구조로 아래와 같이 구성되어 있었습니다.

- 가게 설정 정보 관리 웹 앱(이하 가게관리 웹 앱): 가게 정보 관리, 사장님이 설정한 시간 및 자동 예측 시간 사용 여부, manual 시간 설정 기능 등의 기능을 제공
- Vendor 서비스: BFF 역할, 타 서비스로부터 배달 예측 시간을 요청 받으면 설정과 내부 마이크로서비스들로부터 전달 받은 시간들을 연산하여 제공
- Geography 서비스(이하 Geo 서비스): 가게와 주문자의 위치 정보를 통한 거리 및 지역 정보 등 ML 모델에 전달할 파라미터 일부를 제공하는 역할
- 배달 시간 예측 ML 모델(이하 ML 서비스): ML 팀에서 개발, 서빙만 담당, 각종 예측 시간을 제공하는 역할

1. 각 서비스별 특징 파악

    Vendor 서비스는 외부 타 서비스(가게, 주문, 사장님앱 등)로부터 배달 예측 시간을 전달하는 Proxy 역할을 했습니다.

    또, BFF 역할도 담당하고 있었습니다.

    Geo 서비스는 빠른 연산을 위해 각종 지리 관련 정보를 메모리에 갖고 있었습니다. 이 지리정보 중 일부는 ML 모델에도 공통적으로 사용되는 데이터로 캐시에 넣기에는 용량이 컸습니다.

    데이터를 연산해야 해서 pandas를 내부적으로 사용하고 있었습니다.

    ML 서비스는 최초 서비스 오픈 시에는 학습할 데이터가 없어 일반 모델이 아니었습니다. 당연히 예측 정확도가 떨어져 지속적인 학습과 배포가 필수 요건이었습니다.

    오픈 후 2~3개월 후엔 일반 모델로 교체되었고, 경량화 되었지만 정확도를 높이는 것이 최대 과제였습니다.

    이렇게 각 서비스별 특징을 바탕으로 아래와 같이 접근하기로 하였습니다.

    - 역할에 따른 처리 세분화
        - Vendor 서비스:
            - BFF 역할
            - 가게 정보 상세 조회 및 주문 후 주문정보에서 더 정확한 예측시간 조회 시 호출하는 API 담당
            - 가게 목록 및 검색 목록에 제공되는 대략적인 예측시간 조회 시 호출하는 API 담당
        - ML 서비스:
            - 정확한 예측 시간 담당
            - 대략적인 예측 시간 담당
    - ML 모델의 정확도 vs 처리 시간
        - 정확도를 높이기 위해 지역을 너무 세분화 한 것은 아닌지?
        - 지역 범위를 넓혀 처리 시간을 빠르게 할 경우 정확도는 얼마나 떨어지는지?

2. 인프라 환경과 서비스 특성 파악

    사용하고 있던 GKE의 경우 CPU, 메모리, 디스크등 성능에 따라 Node를 구성할 수 있습니다.

    Pod는 어떤 Node에 위치할 지 지정할 수 있고, Pod가 Provisioning 되는 것은 빠르지만 Node의 경우 Scale-out 되는 시간이 상당히 오래 걸렸습니다.

    배달 앱 특성 상 트래픽은 특정 시간 대에 몰리고, 이 시간 대는 정해져 있었습니다.

## 해결

1. 컨테이너 경량화, 최적화

    각각의 서비스를 컨테이너화 할 때 최대한 경량화 하기 위해 노력했습니다. 컨테이너가 크면 리소스도 많이 차지하지만 Provisioning 시간이 많이 걸려 갑자기 트래픽이 몰리면 처리 시간이 급격하게 늘어나고 Scale Bursting 현상이 발생할 수 있기 때문입니다.

    또, Pod 별 적정한 리소스를 할당하여 자원 최적화를 하기 위해 수 많은 성능 테스트를 했습니다.
    이를 바탕으로 위에 언급한 서비스별 특성과 역할에 맞게 자원을 할당했습니다.

    Vendor 서비스는 CPU 코어를 늘린다고 해서 성능이 크게 좋아지진 않았습니다. 파이썬의 특성 상 쓰레드를 제대로 사용하지 못하여 보통 celery를 이용한 Multiprocessing 방식으로 많이 해결하는데 이렇게 하려면 많은 CPU 코어를 할당해야 하고, 이것은 자원 최적화 및 빠른 Provisioning이 어려워 선택하지 않았습니다.
    가벼운 프레임워크인 FastAPI와 비동기 방식을 최대한 활용하여 경량화와 최적화 했습니다.

2. 용도별 Namespace, Service, Deployment 분리

    같은 Vendor 서비스지만 사용자가 많지 않은 가게 관리 웹 앱의 BFF 역할을 하는 workload와 상대적으로 트래픽이 많지 않은 대략적인 예측시간을 제공하는 workload, 트래픽의 대부분을 차지하고, 더 정확한 예측시간을 제공하는 workload를 분리하여 각각 따로 Scale out 되도록 하였습니다.

3. Scale Out 전략

    Scale Bursting을 막기 위해 두가지 초점을 뒀습니다. 빠른 Provisioning을 위해 Pod에 Probe 설정들을 중점적으로 튜닝했습니다.

    이 때도 각 컨테이너별 특성에 맞게 Liveness Probe 와 Readiness Probe에 사용할 매트릭 선택과 수치를 조정하면서 모니터링을 통해 지속적으로 최적화 했습니다.

    또, GKE Node가 Provisioning 되는 시간이 많이 걸리기 때문에 CronJob을 이용하여 피크 시간대 이전에 미리 Node와 Pod를 늘려 Cold Start와 Scaling Delay를 해결하였습니다.

4. Caching 전략

    배달 앱 특성 상 주문은 인기있는 가게에 몰리게 되어 있습니다.
    또, 메뉴의 카테고리에 따라 주문하는 시간 대가 다릅니다.
    이런 특성을 바탕으로 Vendor 서비스의 경우 L2 캐시로 Redis를 사용했고, Geo 서비스의 경우 L1과 L2 캐시를 복합적으로 사용했습니다.

위와 같은 방법으로 피크 시간 대 p95 기준으로 Latency를 오픈 초 100ms였던 것을 20ms 이하로 만들었습니다.

---

2) 몰입의 순간

- 당신이 온 열정을 쏟아부어 끝까지 파고들었던 프로젝트는 무엇인가요?

- 그 과정에서 배운 점과 성장한 부분을 함께 나눠주세요.

- 당신의 끈기와 열정이 드러나는 이야기를 기다립니다.

=>

다른 많은 프로젝트들도 있지만 위에 언급한 배달 시간 예측 모델 실시간 서빙 서비스 구축 프로젝트가 가장 생각이 납니다.

프로젝트가 시작된 시점에는 제가 속한 조직이 하나의 팀이었는데 조직이 커지면서 프로젝트를 마무리 하는 단계에는 팀장이 되었습니다.

회사 외적/내적으로 다양한 이슈가 있었고, 이로 인해 배차 시스템을 정해진 시간 내에 내재화 해야만 했습니다.
제가 속해 있던 팀은 원래 데이터 조직 내에 있는 서비스 조직이었고, 같은 조직 내에 ML 엔지니어들이 속해 있어 배달 시간 예측 서비스를 개발하게 되었습니다.

타임어택 미션이었고, 마침 제가 속한 팀에서 MLOps 플랫폼을 설계하고 개발하기 위해 PoC를 진행하고 있어 기반이 되는 아키텍처나 기술 스택들을 상당 부분 다뤄봤기 때문에 겨우 시간을 맞출 수 있었습니다.

이런 과정에서 한명의 백엔드 개발자에서 팀장이 됐고, 제가 직접 코딩한 마지막 프로젝트가 되었습니다. (이후엔 동시에 여러 프로젝트를 관리해야 했기 때문에 원치 않았지만 하지 못했습니다.)

요기요 앱에 라이브 서비스 되는 기능이었기에 고가용성과 성능을 보장해야 했기 때문에 모니터링 툴을 이용하여 호출 스택 하나 하나를 뜯어보며 수정, 성능 테스트, 수정을 반복했습니다.

오픈 전 QA팀, 연동되는 다른 서비스를 개발한 팀들과 거의 2주간 매일 새벽에 성능 테스트를 했습니다.
저희 팀은 스테이징 환경에서 테스트가 가능한 환경을 만들어 놓았지만 그 당시 다른 팀들은 스테이징 환경에서 E2E 테스트를 할 수 있는 상태가 아니었기 때문에 운영환경에서 테스트할 수밖에 없어 새벽에 테스트할 수밖에 없었습니다.

이런 어려운 상황 속에서도 미션을 성공시키고야 말겠다는 생각으로 정말 열심히 했던 것 같습니다.
프로젝트 초중반에는 저와 FE 개발자 둘이 진행했었고, 여러 난관도 많았습니다. 혼자 해결하기 어려운 문제들도 많았습니다.
프로젝트 중후반엔 팀원들과 함께 했고, 다양한 아이디어와 팀워크로 문제를 해결할 수 있었습니다.
이 과정에서 팀보다 위대한 개인은 없다는 것을 절실히 느꼈고, 프로젝트의 성공을 이끌어 그들에게 좋은 평가로 보답하고 싶었습니다.